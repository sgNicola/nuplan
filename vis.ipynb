{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from omegaconf import DictConfig \n",
    "from inference.read_report import ReportProcessor\n",
    "from inference.compute_post_scores import ComputePostScore\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from inference.visualization import DataVisualization\n",
    "from inference.performance_statistics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_computation(cfg:DictConfig):\n",
    "    processor = ReportProcessor(cfg)\n",
    "    result_df = processor.read_metric_reports()\n",
    "    InD_scenarios= load_scenario_types_from_csv('scenario_type_counts.csv')\n",
    "    labeled_df = label_scenarios(result_df, InD_scenarios) \n",
    "    compute_postscore=ComputePostScore(labeled_df, cfg)\n",
    "    energy_score = compute_postscore.get_min_score()\n",
    "    average_energy_score = compute_postscore.calculate_average_ood_score()\n",
    "    visualizer = DataVisualization(figsize=(12, 8), alpha=0.6, grid=True)\n",
    "    visualizer.draw_distribution(average_energy_score, score='ood_score_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'inference/config'\n",
    "CONFIG_NAME = 'runner_report'\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "hydra.initialize(config_path=CONFIG_PATH)\n",
    "# cfg = hydra.compose(config_name=CONFIG_NAME,overrides=['split=test','planner=planTF', 'job_name=open_loop_boxes'])\n",
    "cfg = hydra.compose(config_name=CONFIG_NAME,overrides=['job_name=open_loop_boxes'])\n",
    "# cfg = hydra.compose(config_name=CONFIG_NAME)\n",
    "# cfg = hydra.compose(config_name=CONFIG_NAME,overrides=['job_name=closed_loop_reactive_agents'])\n",
    "processor = ReportProcessor(cfg)\n",
    "result_df = processor.read_metric_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=calculate_average_metric_score(result_df)\n",
    "result_df=label_low_score(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following metrics are missing from the DataFrame: ['no_ego_at_fault_collisions', 'drivable_area_compliance', 'driving_direction_compliance', 'time_to_collision_within_bound', 'ego_progress_along_expert_route', 'ego_is_making_progress', 'ego_is_comfortable', 'speed_limit_compliance']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43madd_risk_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 26\u001b[0m, in \u001b[0;36madd_risk_label\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     24\u001b[0m missing_metrics \u001b[38;5;241m=\u001b[39m [metric \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m boolean_metrics \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_metrics:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following metrics are missing from the DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Create the 'risk_label' column: True if any boolean metric < 1, False otherwise\u001b[39;00m\n\u001b[1;32m     29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrisk_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[boolean_metrics] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39many(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The following metrics are missing from the DataFrame: ['no_ego_at_fault_collisions', 'drivable_area_compliance', 'driving_direction_compliance', 'time_to_collision_within_bound', 'ego_progress_along_expert_route', 'ego_is_making_progress', 'ego_is_comfortable', 'speed_limit_compliance']"
     ]
    }
   ],
   "source": [
    "result_df = add_risk_label(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
