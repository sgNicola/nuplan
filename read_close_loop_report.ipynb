{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_metrics =[\n",
    "    'no_ego_at_fault_collisions',  # 0, 0.5,1\n",
    "    'drivable_area_compliance',\n",
    "     'driving_direction_compliance',  # 0, 0.5,1\n",
    "    'time_to_collision_within_bound',\n",
    "    'ego_progress_along_expert_route',\n",
    "    'ego_is_making_progress',\n",
    "    'ego_is_comfortable'\n",
    "]\n",
    "multiplying_metrics =['speed_limit_compliance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF/metrics'\n",
    "def get_report_with_metrics(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'runner_report.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    df = df[['scenario_type', 'scenario_name', 'log_name', 'risk_score']]\n",
    "    # read the metric files\n",
    "\n",
    "    files=['metrics/planner_expert_average_heading_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_average_l2_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_final_heading_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_final_l2_error_within_bound.parquet',\n",
    "    'metrics/planner_miss_rate_within_bound.parquet']\n",
    "\n",
    "    for i in range(len(files)-1):\n",
    "        # get the log_name, scenario_name, and the metric_score columns from the metric files\n",
    "        metric_report = os.path.join(file_path, files[i])\n",
    "        df_metric = pd.read_parquet( metric_report)\n",
    "        df_metric = df_metric[['scenario_name', 'metric_score']]\n",
    "        # merge the runner report and the metric files\n",
    "        df = pd.merge(df, df_metric, on=['scenario_name'])\n",
    "        # rename the metric_score as metric_score_{}.format(i)\n",
    "        df.rename(columns={'metric_score': 'metric_score_{}'.format(i)}, inplace=True)\n",
    "    # print the first 5 rows of the merged DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF'\n",
    "def get_report_with_metrics(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'runner_report.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    df = df[['scenario_type', 'scenario_name', 'log_name', 'risk_score', 'ood_score']]\n",
    "    # read the metric files\n",
    "\n",
    "    files=['metrics/planner_expert_average_heading_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_average_l2_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_final_heading_error_within_bound.parquet',\n",
    "    'metrics/planner_expert_final_l2_error_within_bound.parquet',\n",
    "    'metrics/planner_miss_rate_within_bound.parquet']\n",
    "\n",
    "    for i in range(len(files)-1):\n",
    "        # get the log_name, scenario_name, and the metric_score columns from the metric files\n",
    "        metric_report = os.path.join(file_path, files[i])\n",
    "        df_metric = pd.read_parquet( metric_report)\n",
    "        df_metric = df_metric[['scenario_name', 'metric_score']]\n",
    "        # merge the runner report and the metric files\n",
    "        df = pd.merge(df, df_metric, on=['scenario_name'])\n",
    "        # rename the metric_score as metric_score_{}.format(i)\n",
    "        df.rename(columns={'metric_score': 'metric_score_{}'.format(i)}, inplace=True)\n",
    "    # print the first 5 rows of the merged DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF'\n",
    "def get_report(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'runner_report.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF'\n",
    "def get_report(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'runner_report.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF/metrics'\n",
    "def get_metrics_report(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'speed_limit_compliance.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/sgwang/nuplan/exp/exp/simulation/closed_loop_nonreactive_agents/test_all/planTF'\n",
    "def get_weighted_report(file_path):\n",
    "    # read the runner report\n",
    "    runner_report = os.path.join(file_path, 'closed_loop_nonreactive_agents_weighted_average_metrics_2025.01.04.22.13.32.parquet')\n",
    "    df = pd.read_parquet(runner_report)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
