{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "from ruamel.yaml import YAML\n",
    "from typing import Any, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scenario_filter_from_yaml(yaml_file):\n",
    "    with open(yaml_file, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config.get(\"scenario_types\", [])\n",
    "\n",
    "def get_train_data_from_yaml(yaml_file):\n",
    "    with open(yaml_file, \"r\") as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    train_data = config.get(\"log_splits\", {}).get(\"train\", [])\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_names_from_cache(cache_dir):\n",
    "    log_names = [\n",
    "        folder for folder in os.listdir(cache_dir)\n",
    "        if os.path.isdir(os.path.join(cache_dir, folder))\n",
    "    ]\n",
    "    return log_names\n",
    "\n",
    "def get_all_scenario_tokens(cache_dir):\n",
    "    scenario_tokens = []\n",
    "    for log_name in os.listdir(cache_dir):\n",
    "        log_name_path = os.path.join(cache_dir, log_name)\n",
    "        if os.path.isdir(log_name_path):\n",
    "            for scenario_type in os.listdir(log_name_path):\n",
    "                scenario_type_path = os.path.join(log_name_path, scenario_type)\n",
    "                if os.path.isdir(scenario_type_path):\n",
    "                    for scenario_token in os.listdir(scenario_type_path):\n",
    "                        scenario_token_path = os.path.join(scenario_type_path, scenario_token)\n",
    "                        if os.path.isdir(scenario_token_path):\n",
    "                            scenario_tokens.append(scenario_token)\n",
    "    return scenario_tokens\n",
    "\n",
    "def get_scenario_type_counts(cache_dir):\n",
    "    scenario_type_counts = defaultdict(int)\n",
    " \n",
    "    for log_name in os.listdir(cache_dir):\n",
    "        log_name_path = os.path.join(cache_dir, log_name)\n",
    "        \n",
    "  \n",
    "        if os.path.isdir(log_name_path):\n",
    "            for scenario_type in os.listdir(log_name_path):\n",
    "                scenario_type_path = os.path.join(log_name_path, scenario_type)\n",
    "                \n",
    "                if os.path.isdir(scenario_type_path):\n",
    "                    token_count = len([\n",
    "                        token for token in os.listdir(scenario_type_path)\n",
    "                        if os.path.isdir(os.path.join(scenario_type_path, token))\n",
    "                    ])\n",
    "                    scenario_type_counts[scenario_type] += token_count\n",
    "    return scenario_type_counts\n",
    "\n",
    "def diff_scenario_types(scenario_filter_types, csv_scenario_types):\n",
    "    in_filter_not_in_csv = set(scenario_filter_types) - set(csv_scenario_types)\n",
    "    in_csv_not_in_filter = set(csv_scenario_types) - set(scenario_filter_types)\n",
    "    print(\"Scenario types in scenario_filter but not in CSV:\")\n",
    "    print(in_filter_not_in_csv)\n",
    "    print(\"\\nScenario types in CSV but not in scenario_filter:\")\n",
    "    print(in_csv_not_in_filter)\n",
    "    return in_filter_not_in_csv\n",
    "\n",
    "def get_resample_scenarios(df):\n",
    "    filtered_scenarios = df[df[\"count\"] / 2 < 1000]\n",
    "    scenario_dict = (filtered_scenarios\n",
    "                    .set_index(\"scenario_type\")[\"count\"]\n",
    "                    .apply(lambda x: 1000-(x / 2))  # 对 count 除以 2\n",
    "                    .to_dict())\n",
    "    print(\"Scenario types with count/2 less than 1000 (counts divided by 2):\")\n",
    "    print(scenario_dict)\n",
    "    return scenario_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'stationary': 107944, 'high_magnitude_speed': 30841, 'traversing_intersection': 15792, 'medium_magnitude_speed': 27579, 'traversing_traffic_light_intersection': 45028, 'near_pedestrian_on_crosswalk': 2672, 'on_traffic_light_intersection': 2071, 'near_long_vehicle': 2973, 'low_magnitude_speed': 4902, 'stationary_in_traffic': 36936, 'near_high_speed_vehicle': 3850, 'near_pedestrian_at_pickup_dropoff': 5413, 'on_all_way_stop_intersection': 487, 'starting_protected_noncross_turn': 167, 'stationary_at_traffic_light_without_lead': 7139, 'following_lane_with_slow_lead': 780, 'near_construction_zone_sign': 974, 'following_lane_without_lead': 1418, 'stationary_at_traffic_light_with_lead': 2531, 'stopping_with_lead': 125, 'waiting_for_pedestrian_to_cross': 192, 'following_lane_with_lead': 57, 'near_multiple_pedestrians': 78, 'accelerating_at_traffic_light': 13, 'stopping_at_traffic_light_without_lead': 32, 'near_pedestrian_on_crosswalk_with_ego': 6})\n"
     ]
    }
   ],
   "source": [
    "scenario_type_counts = get_scenario_type_counts(\"exp/cache_pdm_open\")\n",
    "print(scenario_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='exp/resample_cache_pdm_open'\n",
    "scenario_type_count=get_scenario_type_counts(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario types in scenario_filter but not in CSV:\n",
      "{'behind_long_vehicle'}\n",
      "\n",
      "Scenario types in CSV but not in scenario_filter:\n",
      "set()\n",
      "{'behind_long_vehicle'}\n"
     ]
    }
   ],
   "source": [
    "yaml_file = \"InD.yaml\"\n",
    "scenario_filter_types = load_scenario_filter_from_yaml(yaml_file)\n",
    "csv_file = \"open_scenario_type_counts.csv\"  \n",
    "df = pd.read_csv(csv_file)\n",
    "csv_scenario_types = df[\"scenario_type\"].tolist()\n",
    "scenario_to_add = diff_scenario_types(scenario_filter_types, csv_scenario_types)\n",
    "print(scenario_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario types with count/2 less than 1000 (counts divided by 2):\n",
      "{'near_construction_zone_sign': 26.0, 'following_lane_with_slow_lead': 220.0, 'on_all_way_stop_intersection': 513.0, 'waiting_for_pedestrian_to_cross': 808.0, 'starting_protected_noncross_turn': 833.0, 'stopping_with_lead': 875.0, 'near_multiple_pedestrians': 922.0, 'following_lane_with_lead': 943.0, 'stopping_at_traffic_light_without_lead': 968.0, 'accelerating_at_traffic_light': 987.0, 'near_pedestrian_on_crosswalk_with_ego': 994.0}\n"
     ]
    }
   ],
   "source": [
    "resample_scenarios =get_resample_scenarios(df)\n",
    "for scenario in scenario_to_add:\n",
    "    resample_scenarios[scenario] =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resample_scenario_tokens(cache_dir: str, resample_scenarios: Dict[str, float]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Get all scenario tokens for scenario types specified in `resample_scenarios`.\n",
    "\n",
    "    :param cache_dir: Path to the cache directory.\n",
    "    :param resample_scenarios: Dictionary of scenario types to include.\n",
    "    :return: A dictionary where keys are scenario types and values are lists of tokens.\n",
    "    \"\"\"\n",
    "    scenario_tokens = {scenario_type: [] for scenario_type in resample_scenarios.keys()}  # Initialize dictionary\n",
    "\n",
    "    for log_name in os.listdir(cache_dir):\n",
    "        log_name_path = os.path.join(cache_dir, log_name)\n",
    "        if os.path.isdir(log_name_path):\n",
    "            for scenario_type in os.listdir(log_name_path):\n",
    "                # Only process scenario types in resample_scenarios\n",
    "                if scenario_type in resample_scenarios:\n",
    "                    scenario_type_path = os.path.join(log_name_path, scenario_type)\n",
    "                    if os.path.isdir(scenario_type_path):\n",
    "                        for scenario_token in os.listdir(scenario_type_path):\n",
    "                            scenario_token_path = os.path.join(scenario_type_path, scenario_token)\n",
    "                            if os.path.isdir(scenario_token_path):\n",
    "                                # Append the token to the corresponding scenario_type list\n",
    "                                scenario_tokens[scenario_type].append(scenario_token)\n",
    "\n",
    "    return scenario_tokens\n",
    "\n",
    "cache_dir = 'exp/cache_pdm_open'\n",
    "all_exist_tokens=get_resample_scenario_tokens(cache_dir, resample_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(all_exist_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a21b982406725ebd', 'f9c21012e8f65fc4', '2ad5a764d3b85c8f', 'c1763508ac2b5f85', 'd30a370fca7a5d24', '8fae7fcb6fc4581e']\n"
     ]
    }
   ],
   "source": [
    "print(all_exist_tokens.get('near_pedestrian_on_crosswalk_with_ego'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "from typing import Dict, List, Generator\n",
    "from nuplan.database.nuplan_db.nuplan_scenario_queries import (\n",
    "    get_lidarpc_tokens_with_scenario_tag_from_db,\n",
    "    get_sensor_data_token_timestamp_from_db,\n",
    "    get_sensor_token_map_name_from_db,\n",
    ")\n",
    "from collections import defaultdict\n",
    "\n",
    "def execute_many(query: str, params: Tuple, db_file: str):\n",
    "    \"\"\"\n",
    "    Execute a SQL query on a SQLite database and yield the results row by row.\n",
    "    :param query: The SQL query string.\n",
    "    :param params: Parameters for the query.\n",
    "    :param db_file: Path to the SQLite database file.\n",
    "    :yield: Rows from the query result.\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_file) as conn:\n",
    "        conn.row_factory = sqlite3.Row  # Enable dictionary-like row access\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, params)\n",
    "        for row in cursor.fetchall():\n",
    "            yield row\n",
    "\n",
    "def get_scenario_info_from_db(db_file: str, resample_scenarios: Dict[str, float]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Get the scenario tokens for scenario types specified in `resample_scenarios` from a single database file.\n",
    "    \n",
    "    :param db_file: Path to the SQLite database file.\n",
    "    :param resample_scenarios: A dictionary where keys are scenario types and values are desired counts.\n",
    "    :return: A dictionary where keys are scenario types and values are lists of scenario tokens.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT st.type, st.lidar_pc_token\n",
    "    FROM scenario_tag AS st;\n",
    "    \"\"\"\n",
    "    scenario_info = {}\n",
    "    # Execute the query and process the results\n",
    "    for row in execute_many(query, (), db_file):\n",
    "        scenario_type = row[\"type\"]\n",
    "        token = row[\"lidar_pc_token\"]  # This is likely in binary format\n",
    "        # Convert binary token to hex string\n",
    "        token_hex = token.hex() if isinstance(token, bytes) else token\n",
    "        \n",
    "        # Only include scenario_types present in resample_scenarios\n",
    "        if scenario_type in resample_scenarios:\n",
    "            if scenario_type not in scenario_info:\n",
    "                scenario_info[scenario_type] = []\n",
    "            scenario_info[scenario_type].append(token_hex)\n",
    "    \n",
    "    return scenario_info\n",
    "\n",
    "\n",
    "def get_scenario_tokens_from_all_dbs(db_directory: str, db_files: List[str], resample_scenarios: Dict[str, float]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Get scenario tokens for specific scenario types from multiple database files.\n",
    "    \n",
    "    :param db_directory: Directory containing the `.db` files.\n",
    "    :param db_files: List of `.db` files to process.\n",
    "    :param get_resample_scenarios: A dictionary where keys are scenario types and values are desired counts.\n",
    "    :return: A dictionary where keys are scenario types and values are lists of scenario tokens.\n",
    "    \"\"\"\n",
    "    aggregated_scenario_info = {}\n",
    "\n",
    "    for db_file in db_files:\n",
    "        db_path = os.path.join(db_directory, db_file)\n",
    "        if not os.path.isfile(db_path):\n",
    "            continue\n",
    "        scenario_info = get_scenario_info_from_db(db_path, resample_scenarios)\n",
    "\n",
    "        # Merge results from the current database file\n",
    "        for scenario_type, tokens in scenario_info.items():\n",
    "            if scenario_type not in aggregated_scenario_info:\n",
    "                aggregated_scenario_info[scenario_type] = set()\n",
    "            aggregated_scenario_info[scenario_type].update(tokens)\n",
    "            \n",
    "    for scenario_type in aggregated_scenario_info:\n",
    "        aggregated_scenario_info[scenario_type] = list(aggregated_scenario_info[scenario_type])\n",
    "\n",
    "    return aggregated_scenario_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from typing import Dict, List\n",
    "\n",
    "def get_tokens_by_scenario_type(db_file: str, scenario_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the tokens for a specified scenario type from the NuPlan database.\n",
    "\n",
    "    :param db_file: Path to the SQLite database file.\n",
    "    :param scenario_type: The scenario type to filter tokens by.\n",
    "    :return: A list of tokens corresponding to the specified scenario type.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT st.lidar_pc_token\n",
    "    FROM scenario_tag AS st\n",
    "    WHERE st.type = ?;\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    # Connect to the database\n",
    "    with sqlite3.connect(db_file) as conn:\n",
    "        conn.row_factory = sqlite3.Row  # Enable dictionary-like row access\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute the query with the specified scenario type\n",
    "        cursor.execute(query, (scenario_type,))\n",
    "        \n",
    "        # Fetch all matching tokens\n",
    "        for row in cursor.fetchall():\n",
    "            token = row[\"lidar_pc_token\"]\n",
    "            token_hex = token.hex() if isinstance(token, bytes) else token\n",
    "            tokens.append(token_hex)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_directory = os.path.join(os.environ[\"NUPLAN_DATA_ROOT\"], \"nuplan-v1.1/trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = \"nuplan.yaml\"   \n",
    "train_logs = get_train_data_from_yaml(yaml_file)\n",
    "db_files = [\n",
    "    file for file in os.listdir(db_directory)\n",
    "    if file.endswith(\".db\") and os.path.splitext(file)[0] in train_logs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scenario_tokens = get_scenario_tokens_from_all_dbs(db_directory, db_files, resample_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_limit_scenarios(\n",
    "    all_scenario_tokens: Dict[str, List[str]],\n",
    "    all_exist_tokens: Dict[str, List[str]],\n",
    "    resample_scenarios: Dict[str, float]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Filter tokens from `all_scenario_tokens` that do NOT exist in `all_exist_tokens` for the same scenario type,\n",
    "    and limit the number of tokens for each `scenario_type` to the count specified in `resample_scenarios`.\n",
    "\n",
    "    If the target_count is greater than the length of valid_tokens, take all valid_tokens and print the difference.\n",
    "\n",
    "    :param all_scenario_tokens: Dictionary of scenario types and their corresponding tokens.\n",
    "    :param all_exist_tokens: Dictionary of scenario types and their corresponding tokens to exclude.\n",
    "    :param resample_scenarios: Dictionary of scenario types and their target counts.\n",
    "    :return: A dictionary with filtered and limited tokens for each scenario type.\n",
    "    \"\"\"\n",
    "    filtered_scenarios = {}\n",
    "\n",
    "    for scenario_type, tokens in all_scenario_tokens.items():\n",
    "        # Check if the scenario_type is in resample_scenarios\n",
    "        if scenario_type in resample_scenarios:\n",
    "            # Get the desired count for this scenario_type\n",
    "            target_count = int(resample_scenarios[scenario_type])  # Convert float to int\n",
    "\n",
    "            # Get the tokens to exclude for this scenario_type from all_exist_tokens\n",
    "            exclude_tokens = all_exist_tokens.get(scenario_type, [])\n",
    "\n",
    "            # Filter tokens that are NOT in the exclude_tokens list\n",
    "            # valid_tokens = [token for token in tokens if token not in exclude_tokens]\n",
    "            valid_tokens = [token for token in tokens if token not in exclude_tokens]   \n",
    "            # Check if target_count is greater than the length of valid_tokens\n",
    "            if target_count > len(valid_tokens):\n",
    "                # Print the difference\n",
    "                print(f\"Warning: Scenario '{scenario_type}' - target_count ({target_count}) \"\n",
    "                      f\"is greater than valid_tokens length ({len(valid_tokens)}). \"\n",
    "                      f\"Missing {target_count - len(valid_tokens)} tokens.\")\n",
    "\n",
    "                # Take all valid tokens\n",
    "                filtered_scenarios[scenario_type] = valid_tokens\n",
    "            else:\n",
    "                # Limit the number of tokens to the target count\n",
    "                filtered_scenarios[scenario_type] = valid_tokens[:target_count]\n",
    "\n",
    "    return filtered_scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_scenarios = filter_and_limit_scenarios(all_scenario_tokens, all_exist_tokens, resample_scenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_scenarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "def generate_scenario_token_yaml(filtered_scenarios, template_path, output_path):\n",
    "    \"\"\"\n",
    "    Generates a scenario_filter config YAML file with filtered scenarios written under `scenario_tokens`,\n",
    "    while preserving the original format, including null fields, indentation, empty lines, and field order.\n",
    "\n",
    "    :param filtered_scenarios: A dictionary where keys are scenario types and values are lists of tokens.\n",
    "    :param template_path: Path to the template YAML file.\n",
    "    :param output_path: Path to save the generated YAML file.\n",
    "    \"\"\"\n",
    "    # Initialize ruamel.yaml\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True  # Preserve quotes and formatting\n",
    "\n",
    "    # Load the template YAML file\n",
    "    with open(template_path, 'r', encoding='utf-8') as template_file:\n",
    "        scenario_filter_config = yaml.load(template_file)\n",
    "\n",
    "    # Flatten all tokens from filtered_scenarios into a single list\n",
    "    all_tokens = []\n",
    "    for tokens in filtered_scenarios.values():\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # Update the `scenario_tokens` field in the config\n",
    "    # scenario_filter_config['scenario_tokens'] = all_tokens\n",
    "    scenario_filter_config['scenario_tokens'] = [f\"'{token}'\" for token in all_tokens]\n",
    "    # Write the updated YAML back to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        yaml.dump(scenario_filter_config, output_file)\n",
    "\n",
    "    print(f\"YAML file successfully generated at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "def generate_scenario_type_yaml(filtered_scenarios, template_path, output_path):\n",
    "    \"\"\"\n",
    "    Generates a scenario_filter config YAML file with filtered scenarios written under `scenario_tokens`,\n",
    "    while preserving the original format, including null fields, indentation, empty lines, and field order.\n",
    "\n",
    "    :param filtered_scenarios: A dictionary where keys are scenario types and values are lists of tokens.\n",
    "    :param template_path: Path to the template YAML file.\n",
    "    :param output_path: Path to save the generated YAML file.\n",
    "    \"\"\"\n",
    "    # Initialize ruamel.yaml\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True  # Preserve quotes and formatting\n",
    "\n",
    "    # Load the template YAML file\n",
    "    with open(template_path, 'r', encoding='utf-8') as template_file:\n",
    "        scenario_filter_config = yaml.load(template_file)\n",
    "\n",
    "    # Flatten all tokens from filtered_scenarios into a single list\n",
    "    all_types = []\n",
    "    for tokens in filtered_scenarios.keys():\n",
    "        all_types.add(tokens)\n",
    "\n",
    "    # Update the `scenario_tokens` field in the config\n",
    "    # scenario_filter_config['scenario_tokens'] = all_tokens\n",
    "    scenario_filter_config['scenario_types'] = [f\"'{types}'\" for types in all_types]\n",
    "    # Write the updated YAML back to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        yaml.dump(scenario_filter_config, output_file)\n",
    "\n",
    "    print(f\"YAML file successfully generated at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruamel.yaml import YAML\n",
    "\n",
    "def generate_scenario_filter_yaml(filtered_scenarios, template_path, output_path):\n",
    "    \"\"\"\n",
    "    Generates a scenario_filter config YAML file with filtered scenarios written under `scenario_tokens`,\n",
    "    while preserving the original format, including null fields, indentation, empty lines, and field order.\n",
    "\n",
    "    :param filtered_scenarios: A dictionary where keys are scenario types and values are lists of tokens.\n",
    "    :param template_path: Path to the template YAML file.\n",
    "    :param output_path: Path to save the generated YAML file.\n",
    "    \"\"\"\n",
    "    # Initialize ruamel.yaml\n",
    "    yaml = YAML()\n",
    "    yaml.preserve_quotes = True  # Preserve quotes and formatting\n",
    "\n",
    "    # Load the template YAML file\n",
    "    with open(template_path, 'r', encoding='utf-8') as template_file:\n",
    "        scenario_filter_config = yaml.load(template_file)\n",
    "\n",
    "    # Flatten all tokens from filtered_scenarios into a single list\n",
    "    all_types = set()  # Use a set to avoid duplicates\n",
    "    for token in filtered_scenarios.keys():\n",
    "        all_types.add(token)\n",
    "\n",
    "    # Update the `scenario_types` field in the config\n",
    "    scenario_filter_config['scenario_types'] = [f\"'{types}'\" for types in all_types]\n",
    "\n",
    "    # Write the updated YAML back to the output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        yaml.dump(scenario_filter_config, output_file)\n",
    "\n",
    "    print(f\"YAML file successfully generated at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file successfully generated at: resample.yaml\n"
     ]
    }
   ],
   "source": [
    "template_path = 'template.yaml'\n",
    "output_path = 'resample.yaml'             # Path to save the generated YAML file\n",
    "# Generate the YAML\n",
    "generate_scenario_filter_yaml(filtered_scenarios, template_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
